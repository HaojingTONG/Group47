{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4470cdf",
   "metadata": {},
   "source": [
    "# Step1: Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42019fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haojingtong/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 16 variables whereas the saved optimizer has 30 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the file\n",
    "model = load_model('model_weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf76d9",
   "metadata": {},
   "source": [
    "# Step2: Input the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd8cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ead2337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However, Fort Charles was rebuilt as a militar...</td>\n",
       "      <td>Fort Charles was rebuilt as an amusement park ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buchanan's  The Democrats and Republicans have...</td>\n",
       "      <td>THe parties will never be similar.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In order to review an acquisition that is usin...</td>\n",
       "      <td>The auditor only reviews the acquisition itsel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three young people sit outside and engage with...</td>\n",
       "      <td>There is a tin can and string telephone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The lucrative tin mines of Kuala Lumpur in the...</td>\n",
       "      <td>The Chinese labor was seen as less costly and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26939</th>\n",
       "      <td>Information in agencies' plans and reports pro...</td>\n",
       "      <td>Thanks to agencies' plans and reports, over $3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26940</th>\n",
       "      <td>He is the Mr. Magoo of scientific theory, geni...</td>\n",
       "      <td>He understands everything he can't see.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26941</th>\n",
       "      <td>Over the past 25 years, the Postal Service has...</td>\n",
       "      <td>Classifying mail is important to the function ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26942</th>\n",
       "      <td>Whoever first stepped ashore on Madeira discov...</td>\n",
       "      <td>The British discovered the Canary Islands first.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26943</th>\n",
       "      <td>But I admit that when a drum and bugle corps m...</td>\n",
       "      <td>I hate the Star Wars song.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26944 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0      However, Fort Charles was rebuilt as a militar...   \n",
       "1      Buchanan's  The Democrats and Republicans have...   \n",
       "2      In order to review an acquisition that is usin...   \n",
       "3      Three young people sit outside and engage with...   \n",
       "4      The lucrative tin mines of Kuala Lumpur in the...   \n",
       "...                                                  ...   \n",
       "26939  Information in agencies' plans and reports pro...   \n",
       "26940  He is the Mr. Magoo of scientific theory, geni...   \n",
       "26941  Over the past 25 years, the Postal Service has...   \n",
       "26942  Whoever first stepped ashore on Madeira discov...   \n",
       "26943  But I admit that when a drum and bugle corps m...   \n",
       "\n",
       "                                              hypothesis  label  \n",
       "0      Fort Charles was rebuilt as an amusement park ...      0  \n",
       "1                     THe parties will never be similar.      0  \n",
       "2      The auditor only reviews the acquisition itsel...      0  \n",
       "3               There is a tin can and string telephone.      0  \n",
       "4      The Chinese labor was seen as less costly and ...      1  \n",
       "...                                                  ...    ...  \n",
       "26939  Thanks to agencies' plans and reports, over $3...      0  \n",
       "26940            He understands everything he can't see.      0  \n",
       "26941  Classifying mail is important to the function ...      1  \n",
       "26942   The British discovered the Canary Islands first.      0  \n",
       "26943                         I hate the Star Wars song.      0  \n",
       "\n",
       "[26944 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "training_data = 'training_data/NLI/train.csv'\n",
    "df_training = pd.read_csv(training_data)\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7ab212",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = 'training_data/NLI/dev.csv'\n",
    "df_validation = pd.read_csv(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78fe26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation['hypothesis'] = df_validation['hypothesis'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9547d137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.864348277909738\n",
      "10.269521971496436\n",
      "[[  328  2104  1601 ...     0     0     0]\n",
      " [10608     1  1782 ...     0     0     0]\n",
      " [    6   377     5 ...     0     0     0]\n",
      " ...\n",
      " [   83     1   434 ...     0     0     0]\n",
      " [ 8148   108  3633 ...     0     0     0]\n",
      " [   21     8  2768 ...     0     0     0]]\n",
      "26944\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Combine premises & hypotheses for tokenization\n",
    "\n",
    "training_texts = list(df_training['premise']) + list(df_training['hypothesis']) + list(df_validation['premise']) + list(df_validation['premise'])\n",
    "\n",
    "# Initialize and fit the tokenizer\n",
    "training_tokenizer = Tokenizer(num_words=20000)  # Keep the most frequent 10,000words and ignore the rest\n",
    "training_tokenizer.fit_on_texts(training_texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "training_premise_sequences = training_tokenizer.texts_to_sequences(df_training['premise'])\n",
    "training_hypothesis_sequences = training_tokenizer.texts_to_sequences(df_training['hypothesis'])\n",
    "# print(training_premise_sequences)\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "training_max_len = max(max(len(seq) for seq in training_premise_sequences), max(len(seq) for seq in training_hypothesis_sequences))\n",
    "training_max_len = min(training_max_len, 500) # Set a maximum length\n",
    "print(np.mean([len(seq) for seq in training_premise_sequences]))\n",
    "print(np.mean([len(seq) for seq in training_hypothesis_sequences]))\n",
    "\n",
    "# print(max_len)\n",
    "\n",
    "# Pad sequences\n",
    "training_premise_padded = pad_sequences(training_premise_sequences, maxlen=40, padding='post', truncating='post')\n",
    "training_hypothesis_padded = pad_sequences(training_hypothesis_sequences, maxlen=40, padding='post', truncating='post')\n",
    "\n",
    "print(training_premise_padded)\n",
    "print(len(training_premise_padded))\n",
    "print(len(training_premise_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163eb339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.724506456879915\n",
      "10.176488051061304\n",
      "[[ 6445 16141     0 ...     0     0     0]\n",
      " [   24    91  2072 ...     0     0     0]\n",
      " [  874    38   474 ...     0     0     0]\n",
      " ...\n",
      " [ 1566   472     4 ...     0     0     0]\n",
      " [   21     5  4408 ...     0     0     0]\n",
      " [   71  1298     5 ...     0     0     0]]\n",
      "6737\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Combine premises & hypotheses for tokenization\n",
    "\n",
    "# validation_texts = list(df_validation['premise']) + list(df_validation['hypothesis'])\n",
    "# validation_texts = [str(text) for text in validation_texts]\n",
    "\n",
    "# # Initialize and fit the tokenizer\n",
    "# validation_tokenizer = Tokenizer(num_words=10000)  # Keep the most frequent 10,000words and ignore the rest\n",
    "# validation_tokenizer.fit_on_texts(validation_texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "validation_premise_sequences = training_tokenizer.texts_to_sequences(df_validation['premise'])\n",
    "validation_hypothesis_sequences = training_tokenizer.texts_to_sequences(df_validation['hypothesis'])\n",
    "# print(validation_hypothesis_sequences)\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "validation_max_len = max(max(len(seq) for seq in validation_premise_sequences), max(len(seq) for seq in validation_hypothesis_sequences))\n",
    "validation_max_len = min(validation_max_len, 500) # Set a maximum length\n",
    "print(np.mean([len(seq) for seq in validation_premise_sequences]))\n",
    "print(np.mean([len(seq) for seq in validation_hypothesis_sequences]))\n",
    "\n",
    "# print(max_len)\n",
    "\n",
    "# Pad sequences\n",
    "validation_premise_padded = pad_sequences(validation_premise_sequences, maxlen=40, padding='post', truncating='post')\n",
    "validation_hypothesis_padded = pad_sequences(validation_hypothesis_sequences, maxlen=40, padding='post', truncating='post')\n",
    "\n",
    "print(validation_premise_padded)\n",
    "print(len(validation_premise_padded))\n",
    "print(len(validation_premise_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1463f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a8e54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n",
      "26944\n",
      "[0 0 1 ... 1 0 1]\n",
      "6737\n"
     ]
    }
   ],
   "source": [
    "# Getting the labels\n",
    "\n",
    "# training_labels = df_training['label'].values\n",
    "# If labels are not integers\n",
    "training_labels = df_training['label'].astype(int).values\n",
    "validation_labels = df_validation['label'].astype(int).values\n",
    "\n",
    "print(training_labels)\n",
    "print(len(training_labels))\n",
    "print(validation_labels)\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b41b06",
   "metadata": {},
   "source": [
    "# Step3: Add Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0c7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'test_data/NLI/test.csv'\n",
    "df_test = pd.read_csv(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64cb87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.79588128407026\n",
      "6.902786190187765\n",
      "[[ 291  236  253 ...    0    0    0]\n",
      " [   2 2024  138 ...    0    0    0]\n",
      " [ 139   47    6 ...    0    0    0]\n",
      " ...\n",
      " [  13   48  106 ...    0    0    0]\n",
      " [   2  799    4 ...    0    0    0]\n",
      " [   2   64  236 ...    0    0    0]]\n",
      "3302\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences\n",
    "test_premise_sequences = training_tokenizer.texts_to_sequences(df_test['premise'])\n",
    "test_hypothesis_sequences = training_tokenizer.texts_to_sequences(df_test['hypothesis'])\n",
    "# print(validation_hypothesis_sequences)\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "test_max_len = max(max(len(seq) for seq in test_premise_sequences), max(len(seq) for seq in test_hypothesis_sequences))\n",
    "test_max_len = min(test_max_len, 500) # Set a maximum length\n",
    "print(np.mean([len(seq) for seq in test_premise_sequences]))\n",
    "print(np.mean([len(seq) for seq in test_hypothesis_sequences]))\n",
    "\n",
    "# print(max_len)\n",
    "\n",
    "# Pad sequences\n",
    "test_premise_padded = pad_sequences(test_premise_sequences, maxlen=40, padding='post', truncating='post')\n",
    "test_hypothesis_padded = pad_sequences(test_hypothesis_sequences, maxlen=40, padding='post', truncating='post')\n",
    "\n",
    "print(test_premise_padded)\n",
    "print(len(test_premise_padded))\n",
    "print(len(test_premise_padded[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80442ad9",
   "metadata": {},
   "source": [
    "# Step4: Generate the prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73aac1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "[[0.37312186 0.6268781 ]\n",
      " [0.50631595 0.49368402]\n",
      " [0.24171114 0.75828886]\n",
      " ...\n",
      " [0.39328644 0.6067135 ]\n",
      " [0.5653148  0.43468523]\n",
      " [0.5212724  0.47872755]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your trained model and 'validation_hypothesis_padded', 'validation_premise_padded' are your validation data\n",
    "\n",
    "# 1. Restore best weights (already done during training)\n",
    "# 2. Make predictions on the validation set\n",
    "predictions = model.predict([test_hypothesis_padded, test_premise_padded])\n",
    "\n",
    "print(predictions)\n",
    "# Assuming you have original labels, combine predictions with original data\n",
    "data_with_predictions = pd.DataFrame({\n",
    "#     'Original_Labels': validation_labels,\n",
    "    'Predictions': np.argmax(predictions, axis=1)\n",
    "})\n",
    "\n",
    "# Write data with predictions to CSV\n",
    "data_with_predictions.to_csv('Group_47_B.csv', index=False)\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# val_accuracy = accuracy_score(validation_labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "# print(\"val Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1be955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
